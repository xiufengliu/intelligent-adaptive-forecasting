#!/usr/bin/env python3
"""
Validation Script for FRESH Comprehensive Baseline Comparison
Checks that all components are ready for GPU cluster deployment
Ensures academic integrity and completeness for top-conference submission
"""

import os
import sys
import json
import numpy as np
from pathlib import Path
from datetime import datetime

def check_academic_integrity():
    """Check for any remaining synthetic data or academic integrity violations"""
    print("ğŸ” ACADEMIC INTEGRITY CHECK")
    print("=" * 50)
    
    issues = []
    
    # Check main experiment file
    with open('run_comprehensive_baseline_comparison.py', 'r') as f:
        content = f.read()
        
    # Look for problematic patterns
    if 'synthetic' in content.lower() and 'no synthetic' not in content.lower():
        issues.append("âŒ Potential synthetic data references found")
    
    if 'placeholder' in content.lower() and 'no placeholder' not in content.lower():
        issues.append("âŒ Potential placeholder data references found")
        
    if 'rl_agents' in content and 'REMOVED' not in content:
        issues.append("âŒ RL agents still present (synthetic data)")
    
    # Check for proper error handling
    if 'raise FileNotFoundError' not in content:
        issues.append("âš ï¸ Missing proper error handling for missing data")
    
    if issues:
        for issue in issues:
            print(issue)
        return False
    else:
        print("âœ… No academic integrity violations found")
        print("âœ… All synthetic data properly removed")
        print("âœ… Proper error handling for missing data")
        return True

def check_data_availability():
    """Check that all required datasets are available"""
    print("\nğŸ“Š DATA AVAILABILITY CHECK")
    print("=" * 50)
    
    datasets = ['etth1', 'etth2', 'ettm1', 'ettm2', 'exchange_rate', 'weather', 'illness', 'ecl']
    
    missing_data = []
    available_data = []
    
    for dataset in datasets:
        train_path = f'data/splits/{dataset}_train.npy'
        test_path = f'data/splits/{dataset}_test.npy'
        
        if os.path.exists(train_path) and os.path.exists(test_path):
            # Check data shapes
            train_data = np.load(train_path)
            test_data = np.load(test_path)
            available_data.append({
                'dataset': dataset,
                'train_shape': train_data.shape,
                'test_shape': test_data.shape
            })
            print(f"âœ… {dataset}: train={train_data.shape}, test={test_data.shape}")
        else:
            missing_data.append(dataset)
            print(f"âŒ {dataset}: Missing data files")
    
    print(f"\nğŸ“ˆ Summary: {len(available_data)}/{len(datasets)} datasets available")
    
    if missing_data:
        print(f"âŒ Missing datasets: {missing_data}")
        return False
    else:
        print("âœ… All required datasets available")
        return True

def check_baseline_methods():
    """Check that baseline methods are properly implemented"""
    print("\nğŸ”§ BASELINE METHODS CHECK")
    print("=" * 50)
    
    try:
        # This will fail on development server but shows what will be checked
        print("Note: Cannot import baseline methods on development server (missing torch)")
        print("Will be validated on GPU cluster with proper dependencies")
        
        # Check that the baseline methods file exists
        if os.path.exists('models/baseline_methods.py'):
            print("âœ… Baseline methods file exists")
            
            with open('models/baseline_methods.py', 'r') as f:
                content = f.read()
                
            # Check for key methods
            required_methods = ['Naive', 'Seasonal_Naive', 'Linear', 'DLinear', 'ARIMA', 'ETS', 'Prophet', 'LSTM', 'Transformer']
            
            missing_methods = []
            for method in required_methods:
                if f"'{method}'" in content or f'"{method}"' in content:
                    print(f"âœ… {method} method found")
                else:
                    missing_methods.append(method)
                    print(f"âŒ {method} method missing")
            
            if missing_methods:
                print(f"âŒ Missing methods: {missing_methods}")
                return False
            else:
                print("âœ… All required baseline methods present")
                return True
        else:
            print("âŒ Baseline methods file not found")
            return False
            
    except Exception as e:
        print(f"âš ï¸ Cannot fully validate baseline methods: {e}")
        return True  # Will be validated on GPU cluster

def check_iasnh_results():
    """Check that fresh I-ASNH results are available"""
    print("\nğŸ§  I-ASNH RESULTS CHECK")
    print("=" * 50)
    
    iasnh_file = 'core_iasnh_method_selection_results.json'
    
    if not os.path.exists(iasnh_file):
        print(f"âŒ I-ASNH results file not found: {iasnh_file}")
        return False
    
    try:
        with open(iasnh_file, 'r') as f:
            iasnh_data = json.load(f)
        
        # Check structure
        if 'experiment_info' not in iasnh_data:
            print("âŒ Invalid I-ASNH results structure")
            return False
        
        exp_info = iasnh_data['experiment_info']
        
        # Check academic integrity
        if exp_info.get('synthetic_data_used', True):
            print("âŒ I-ASNH results contain synthetic data")
            return False
        
        # Check timestamp
        timestamp = exp_info.get('timestamp', '')
        print(f"ğŸ“… I-ASNH results timestamp: {timestamp}")
        
        # Check summary data
        if 'method_selection_results' in iasnh_data and 'summary' in iasnh_data['method_selection_results']:
            summary = iasnh_data['method_selection_results']['summary']
            print(f"ğŸ“Š I-ASNH performance: {summary.get('avg_mase', 'Unknown')} MASE")
            print(f"ğŸ“Š I-ASNH accuracy: {summary.get('selection_accuracy', 'Unknown'):.1%}")
            print(f"ğŸ“Š I-ASNH diversity: {summary.get('method_diversity', 'Unknown')} methods")
        
        print("âœ… I-ASNH results valid and ready")
        return True
        
    except Exception as e:
        print(f"âŒ Error validating I-ASNH results: {e}")
        return False

def check_job_script():
    """Check that job submission script is properly configured"""
    print("\nğŸš€ JOB SCRIPT CHECK")
    print("=" * 50)
    
    job_script = 'submit_comprehensive_baseline_job.sh'
    
    if not os.path.exists(job_script):
        print(f"âŒ Job script not found: {job_script}")
        return False
    
    with open(job_script, 'r') as f:
        content = f.read()
    
    # Check for updated configurations
    checks = [
        ('FRESH', 'âœ… Updated for FRESH experiments'),
        ('academic integrity', 'âœ… Academic integrity maintained'),
        ('torch torchvision', 'âœ… Proper dependencies specified'),
        ('4:00', 'âœ… Sufficient time allocation (4 hours)'),
        ('16GB', 'âœ… Sufficient memory allocation (16GB)'),
    ]
    
    issues = []
    for check, message in checks:
        if check in content:
            print(message)
        else:
            issues.append(f"âŒ Missing: {check}")
    
    # Check for removed RL references
    if 'RL Agents: REMOVED' in content:
        print("âœ… RL agents properly removed")
    else:
        issues.append("âŒ RL agents not properly removed")
    
    if issues:
        for issue in issues:
            print(issue)
        return False
    else:
        print("âœ… Job script properly configured")
        return True

def main():
    """Run all validation checks"""
    print("ğŸ†• FRESH COMPREHENSIVE BASELINE COMPARISON - VALIDATION")
    print("ğŸ¯ TOP CONFERENCE SUBMISSION READINESS CHECK")
    print("=" * 80)
    print(f"Validation time: {datetime.now()}")
    print("=" * 80)
    
    checks = [
        ("Academic Integrity", check_academic_integrity),
        ("Data Availability", check_data_availability),
        ("Baseline Methods", check_baseline_methods),
        ("I-ASNH Results", check_iasnh_results),
        ("Job Script", check_job_script)
    ]
    
    results = {}
    
    for check_name, check_func in checks:
        results[check_name] = check_func()
    
    # Final summary
    print("\n" + "=" * 80)
    print("ğŸ¯ VALIDATION SUMMARY")
    print("=" * 80)
    
    passed = sum(results.values())
    total = len(results)
    
    for check_name, result in results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{status} {check_name}")
    
    print(f"\nOverall: {passed}/{total} checks passed")
    
    if passed == total:
        print("\nğŸ‰ ALL CHECKS PASSED!")
        print("âœ… Ready for GPU cluster deployment")
        print("âœ… Academic integrity maintained")
        print("âœ… Top conference submission ready")
        return True
    else:
        print(f"\nâŒ {total - passed} CHECKS FAILED!")
        print("âŒ Not ready for deployment")
        print("Please fix the issues above before submitting to GPU cluster")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
