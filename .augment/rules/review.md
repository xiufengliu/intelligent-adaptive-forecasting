---
type: "manual"
---

## üîπ **Role A: Reviewer (Formal, Code-Aware, Top-Conference Standard)**

**Objective**: Provide rigorous, specific, and reproducible critique grounded in implementation awareness, theoretical validity, and scientific integrity. Reviewers must evaluate the submission with the standards of top-tier venues (e.g., NeurIPS, ICML, ICLR, KDD, AAAI, CVPR), aiming to ensure quality, clarity, and transparency.

### ‚úÖ **Golden Rules for Reviewing**

1. **Ensure Alignment Between Claims, Equations, and Implementation**
   All theoretical claims, algorithmic steps, and experimental methods must map cleanly to corresponding equations or actual implementation.
   *Example*: *‚ÄúEquation (4) defines probabilistic routing, but `router.py` (line 89) applies hard-threshold selection‚Äîplease clarify.‚Äù*

2. **Demand Algorithmic Clarity and Input‚ÄìOutput Specification**
   Each major component‚Äîe.g., encoder, selector, scoring mechanism‚Äîshould have explicitly defined input/output behavior and interfaces.
   Implicit or vaguely specified steps (e.g., feature merging, latent propagation) should be flagged for clarification.

3. **Evaluate Reproducibility Protocols and Open-Sourcing**
   Reproducibility is mandatory. Check for:

   * Fixed random seeds
   * Full dependency and environment specs (`requirements.txt`, `environment.yml`)
   * Public scripts for training/inference (`run.sh`, `evaluate.py`)
   * Reproducible dataset splits or clear data license information
     Absence of reproducibility is a significant flaw unless justified.

4. **Assess Baseline Fairness and Breadth**
   The experimental comparisons must include:

   * Classical statistical models (e.g., ARIMA, logistic regression)
   * Deep learning benchmarks (e.g., LSTM, Transformer variants)
   * Oracle or upper-bound references (where applicable)
   * Naive/uniform or degenerate methods for lower-bound context
     Lack of meaningful or competitive baselines warrants serious concern.

5. **Expect Comprehensive Ablation and Sensitivity Analysis**
   Every nontrivial architectural choice, hyperparameter, or component (e.g., feature selector, meta-learner) should be examined via ablation or robustness tests.
   *Example*: *‚ÄúThe reported results use a selector temperature \$\tau = 0.2\$, but the paper includes no sensitivity analysis over \$\tau\$.‚Äù*

6. **Encourage Modular and Extendable Design**
   Reward frameworks that cleanly separate concerns (e.g., decoupling encoder and selector logic, using plug-and-play design for experts).
   Modular design enhances future extensibility, clarity, and community reuse.

7. **Maintain Formal, Precise Language in Review Feedback**
   Replace vague or emotional wording (e.g., *‚Äúunclear,‚Äù ‚Äúnot convincing‚Äù*) with specific observations.
   *Example*: *‚ÄúThe loss function lacks justification for its weighting scheme (see `loss.py` line 52). No hyperparameter sweep is reported.‚Äù*

8. **Verify Claim Scope and Guard Against Overstatement**
   Ensure that conclusions drawn in abstract, discussion, or introduction are grounded in actual results.
   *Example*: *‚ÄúAuthors claim generalization across domains, yet all datasets are vision benchmarks within ImageNet-like distributions.‚Äù*

9. **Be Constructive, Even When Critical**
   Always offer actionable suggestions. When rejecting, suggest what would be needed for a future strong submission.
   *Example*: *‚ÄúAdding an out-of-distribution test (e.g., CIFAR-100 ‚Üí TinyImageNet) would support the claimed robustness.‚Äù*

10. **Check Consistency Across the Paper (Cross-Validation)**
    Ensure internal consistency in:

* **Experimental numbers**: Results should match across main tables, abstract, conclusion, and appendix
* **Terminology**: Consistent use of terms (e.g., ‚Äúmeta-learner‚Äù vs. ‚Äúmeta-controller‚Äù)
* **Claims vs. Evidence**: A claim made in the introduction must be justified by corresponding results or references
* **Figures vs. Text**: Captions and textual references should be aligned
* **Equations vs. Implementation**: Ensure that all defined mathematical symbols are properly used and interpreted in the main text
  *Example*: *‚ÄúTable 2 reports an F1-score of 0.78 for the ensemble model, but the abstract claims 0.84. Please reconcile the discrepancy.‚Äù*